---
title: 언어모델, BERT 알아보기
author: JONGSKY
date: 2020-09-25 17:25:00
categories: [NLP]
tags: [bert, nlp]
toc: false
---

한국어 기사 요약 프로젝트를 시작하며 프로젝트에 도입해야할 여러 논문들과 연구들을 찾았습니다. 그중에서도 언어모델로 유명한 ```BERT``` 모델에 대해 정리하면 좋을 것 같다고 생각하여 작성하게 되었습니다. 

![image](https://user-images.githubusercontent.com/40276516/94236173-bd2faa80-ff47-11ea-9262-184f550b6cbb.png)

## 언어모델 BERT 란

구글에서 개발한 인공지능(AI) 언어모델 ```BERT```(이하 버트, Bidirectional Encoder Representations from Transformers)는 **NLP(자연어처리) 사전 훈련 기술**이며, 특정 분야에 국한된 기술이 아닌 모든 자연어 처리 분야에서 좋은 성능을 내는 **범용 Language Model** 입니다. 11개 이상의 자연어처리 과제에서 ```BERT```가 최첨단 성능을 발휘한다고 하지만 그 이유는 잘 알려져 있지 않다고 합니다. 하지만 ```BERT```는 지금까지 자연어처리에 활용하였던 앙상블 모델보다 더 좋은 성능을 내고 있어 많은 관심을 받고 있는 언어모델입니다. 

```BERT```등장 이전에는 데이터의 전처리 임베딩을 Word2Vec, GloVe, Fasttext 방식을 많이 사용했지만, 등장 이후 고성능을 내는 대부분의 모델에서는 ```BERT```를 많이 사용하고 있다고 합니다. 임베딩 과정에서 ```BERT```를 사용하는 것이고, ```BERT```는 특정 모델(분류, 인식 등)를 시작 하기 전 **사전 훈련 Embedding을 통해 특정 모델의 성능을 더 좋게 할 수 있는 언어모델입니다.**

## BERT의 내부 동작

### 1. Input

![image](https://user-images.githubusercontent.com/40276516/94236526-58c11b00-ff48-11ea-8b85-822ebffd3d11.png)

```BERT```는 위 그림과 같이 **3가지의 입력 임베딩(Token, Segment, Position 임베딩)의 합으로 구성된다.**

#### 1-1. Token Embeddings

```BERT```는 기존에 사용되던 워드 임베딩 방식의 임베딩을 사용하지 않았습니다. Word Piece 임베딩 방식을 사용핬는데 이 방식은 자주 등장하면서 가장 긴 길이의 sub-word를 하나의 단위로 만듭니다. 즉, 자주 등장하는 sub-word는 그 자체가 단위가 되고, 자주 등장하지 않는 단어(rare word)는 sub-word로 쪼개지게 됩니다. 기존 워드 임베딩 방법은 Out-of-vocabulary(OOV) 문제가 있었습니다. 희기 단어, 이름, 숫자 나 단어장에 없는 단어에 대한 학습, 번역에 어려움이 있었습니다. 그러나 Word Piece 임베딩은 모든 언어에 적용 가능하며 sub-word 단위로 단어를 분절하므로 OOV 처리에 효과적이고 정확도도 상승하는 효과가 있습니다.

#### 1-2. Segment Embeddings

```BERT```는 두 개의 문장을 문장 구분자 ([SEP])와 함께 합쳐 넣습니다. 입력 길이의 제한으로 두 문장은 합쳐서 512 subword 이하로 제한합니다. 입력의 길이가 길어질수록 학습시간은 제곱으로 증가하기 때문에 적절한 입력 길이를 설정해야 합니다. 한국어는 보통 평균 20 subword로 구성되고 99%가 60 subword를 넘지 않기 때문에 입력 길이를 두 문장이 합쳐 128로 해도 충분합니다. 하지만 간혹 긴 문장이 있으므로 우선 입력 길이 128로 제한하고 학습한 후, 128보다 긴 입력들을 모아 마지막에 따로 추가 학습하는 방식을 사용합니다.

#### 1-3. Position Embedding

```BERT```는 저자의 이전 논문인 Transformer 모델을 사용하였습니다. Transformer는 주로 사용하는 CNN, RNN 모델을 사용하지 않고 Self-Attention 모델을 사용합니다. Self-Attention은 입력의 위치에 대해 고려하지 못하므로 입력 토큰의 위치 정보를 줘야합니다. 그래서 Transformer에서는 Sinusoid 함수를 이용한 Positional encoding을 사용하였고, ```BERT```에서는 이를 변형하여 Position encoding을 사용합니다. Position encoding은 단순하게 Token 순서대로 0,1,2,3 ... 과 같이 순서대로 인코딩합니다.

```BERT```는 위 세가지 임베딩을 합치고 이에 Layer정규화와 Dropout을 적용하여 입력으로 사용합니다.

### 2. 언어 모델링 구조 (Pre-training BERT)

![image](https://user-images.githubusercontent.com/40276516/94240535-611c5480-ff4e-11ea-8106-653ac7a0511f.png)

데이터 임베딩 후 훈련시킬 데이터를 인코딩 하였으면, Pre-training(사전훈련)을 시킬 단계입니다. 기존의 방법들은 보통 문장을 왼쪽에서 오른쪽으로 학습하여 다음 단어를 예측하는 방식이거나, 예측할 단어의 좌우 문맥을 고려하여 예측하는 방식을 사용합니다. 그러나 ```BERT```는 언어의 특성이 잘 학습되도록 2가지 방식, **MLM(Masked Language Model)과 NSP(Next Sentence Prediction)을 사용합니다.**

![image](https://user-images.githubusercontent.com/40276516/94241593-ce7cb500-ff4f-11ea-9556-d7f2a0b8e6a5.png)

논문에서는 기존(좌에서 우로 학습하는 모델)방식과 비교하여 MLM방식이 더 좋은 성능을 보여주고 있다고 말합니다.


#### 2-1. 언어 모델링 데이터

```BERT```는 총 33억개 단어(BookCorpus 8억개 단어 + Wikipedia 25억개 단어)의 거대한 말뭉치를 이용하여 학습합니다. 건대한 말뭉치를 MLM, NSP 모델 적용을 위해 **스스로 라벨을 만들고 수행하여 준지도학습이라고 합니다.**

#### 2-2. 모델 구조

![image](https://user-images.githubusercontent.com/40276516/94242512-18b26600-ff51-11ea-8b84-92d9726ef4a1.png)

```BERT``` 모델은 Transformer를 기반으로 합니다. 위 그림은 Transformer 모델 구조로 인코더-디코더 모델이며 번역 도메인에서 최고 성능을 기록하였습니다. 기존 인코더-디코더 모델들과 다르게 Transformer는 CNN, RNN을 이용하지 않고 Self-attetion이라는 개념을 도입하였습니다. ```BERT```는 Transformer의 인코더-디코더 중 인코더만 사용하는 모델입니다.

```BERT```는 2가지 버전이 있습니다. 
- BERT-base (L=12, H=768, A=12)
- BERT-large (L=24, H=1024, A=16)

**L은 Transformer 블록의 숫자이고 H는 hidden size, A는 Transformer의 Attetion block 숫자입니다.** 즉 L,H,A가 크다는 것은 블록을 많이 쌓았고, 표현하는 은닉층이 크며 Attetion 개수를 많이 사용하였다는 뜻입니다. BERT-base는 1.1억개의 학습 파라미터, BERT-large는 3.4억개의 학습 파라미터가 있습니다. BERT는 학습파라미터가 매우 많기때문에 학습시간이 무척 오래걸립니다.

#### 2-3. MLM (Masked Language Model)

![image](https://user-images.githubusercontent.com/40276516/94241805-18659b00-ff50-11ea-97a9-ce9880d1576b.png)

: 입력 문장에서 임의로 토큰을 버리고(Mask), 그 토큰을 맞추는 방식으로 학습을 진행합니다.

#### 2-4. NSP (Next Sentence Prediction)

![image](https://user-images.githubusercontent.com/40276516/94241907-421ec200-ff50-11ea-9787-6e96af4fd287.png)

: 두 문장이 주어졌을 때, 두 문장의 순서를 예측하는 방식입니다. 두 문장 간 관련이 고려되야 하는 NLI와 QA의 파인 튜닝을 위해 두 문장의 연관을 맞추는 학습을 진행합니다.

### 3. 학습된 언어모델 전이학습 (Transfer Learning)

![image](https://user-images.githubusercontent.com/40276516/94243666-ab073980-ff52-11ea-94af-bc15226107a1.png)

실질적으로 성능이 관칠되는 것은 전이학습이지만, 언어 모델이 제대로 학습되어야 전이학습 시 좋은 성능이 나옵니다. 기존 알고리즘들은 자연어의 다양한 Task에 가각의 알고리즘을 독립적으로 만들어야 했습니다. 하지만 ```BERT``` 개발 이후 많은 자연어처리 연구자들은 언어 모델을 만드는데 더 공을 들이게 되었습니다. 그리고 전이학습 Task의 성능도 훨씬 더 좋아졌습니다. **전이학습은 라벨이 주어지므로 지도학습(Supervised Learning)입니다.**

전이학습은 ```BERT```의 언어 모델의 출력에 추가적인 모델을 쌓아 만듭니다. 일반적으로 복잡한 CNN, LSTM, Attetion을 쌓지 않고 간단한 DNN만 쌓아도 성능이 잘 나오며 별 차이가 없다고 알려져 있습니다.

## BERT의 한계점

```BERT```는 일반 NLP모델에서 잘 작동하지만, Bio, Science, Finace 등 특정 분야의 언어모델에 사용하려면 잘 적용되지 않는다고 합니다. 사용단어들이 다르고 언어의 특성이 다르기 때문이라고 합니다. 따라서 특정 분야에 대해 ```BERT```를 적용하려면 특정 분야의 특성을 수집할 수 있는 언어데이터들을 수집하고, 언어모델 학습을 추가적으로 진행해주어야 합니다.

## 결론

```BERT```는 18년 10월 처음 발표된 이래로 조금씩 변형된 수많은 모델들이 쏟아져 나오고 있습니다. 구글, Facebbok, MS 등과 같이 엄청난 리소스와 인력으로 일반 기업에서는 시도도 못할 정도입니다. 점점 자연어 처리 분야가 리소스가 없으면 접근도 할 수 없이 진입장벽이 높아지는 것 같다고 많이 말합니다.

```BERT```의 성능을 능가하기 위해 더 큰 모델과 학습 데이터를 이용한 XLNet, RoBERTa 연구가 진행되고 학습시간을 줄이기 위해 Floating point 16을 사용하는 방법이나, 모델을 경량화 하는 ALBERT, Knowledge distillation 연구도 활발하게 진행되고 있습니다.

빠르게 발전하는 자연어 처리 모델을 공부하는 것도 중요하지만 논문을 정확히 이해하고 습득할 수 있어야 한다고 생각합니다. 또한 실제 운용을 위한 모델 경량화, 자신의 데이터에 맞는 적절한 전처리 작업이 훨씬 중요하다고 말합니다.

### 출처

- [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/pdf/1810.04805.pdf)
- [BERT 돌아보기](http://docs.likejazz.com/bert/)
- [The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning)](http://jalammar.github.io/illustrated-bert/)
- [BERT에 대해 쉽게 알아보기1](https://ebbnflow.tistory.com/151)
- [인공지능(AI)언어모델 BERT(버트)는 무엇인가](http://www.aitimes.kr/news/articleView.html?idxno=13117)
- [BERT를 파해쳐 보자!!](https://keep-steady.tistory.com/19)
